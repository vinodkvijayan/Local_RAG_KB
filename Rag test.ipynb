{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3056b671-4040-4607-a89f-ca5417fd2a38",
   "metadata": {},
   "source": [
    "**Building a Local RAG Document Knowledge Base with Ollama, LlamaIndex, and ChromaDB**\n",
    "\n",
    "---\n",
    "\n",
    "This article outlines the process of building a local Retrieval Augmented Generation (RAG) document knowledge base using Ollama for local Large Language Model (LLM) inference, LlamaIndex for data ingestion and indexing, and ChromaDB for vector storage.  This approach allows for efficient querying of a private document corpus without relying on external LLM APIs, enhancing privacy and reducing costs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1e044b-2887-4af0-9854-0046b11e4fef",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "**Retrieval Augmented Generation (RAG)** combines the strengths of LLMs with external knowledge sources.  Instead of relying solely on the LLM's pre-trained knowledge, RAG retrieves relevant information from a knowledge base before generating a response. This allows the LLM to provide more accurate, contextually grounded, and up-to-date answers.  This paper focuses on building a local RAG pipeline, meaning all components run on your machine, offering increased data privacy and control.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7312d6-df4a-4c92-b9fd-44099c496416",
   "metadata": {},
   "source": [
    "> This setup was tried and tested on a **Dell Latitude 5430 Laptop** , with following specification\n",
    "> > 1. **CPU** - 12th Gen Intel® Core™ i7-1256U vPro® *(12 MB Intel® Smart Cache, 10 cores, 12 threads, up to 4.80 GHz Turbo)*\n",
    "> > 2. **GPU** -  Integrated Intel® Iris® Xe Graphics 1.25 GHz - * 8GB Shared Memory*\n",
    "> > 3. **RAM** - 16 GB, 2 x 8 GB, DDR4, 3200 MT/s, Non-ECC, dual-channel\n",
    "> > 4. **OS** - Windows 11 Enterprise version 23H2 *build 22631.4751*\n",
    "> > 5. **Conda** version 24.9.1 environment with following : \n",
    "> > > 1. **Python** - Python 3.11.0 | *packaged by Anaconda, Inc.|(main, Mar  1 2023, 18:18:21) [MSC v.1916 64 bit (AMD64)] on win32*\n",
    "> > > 2. **intel IPEX-LLM** for Ollama [***pip install --pre --upgrade ipex-llm[cpp]*** ]\n",
    "> > > 3. **llama-index**                              0.12.19 , ***with additional integrations as below***\n",
    "> > > > 1. **llama-index-vector-stores-chroma**         0.4.1\n",
    "> > > > 2. **llama-index-llms-ollama**                  0.5.2\n",
    "> > > > 3. **llama-index-embeddings-huggingface**      0.5.1\n",
    "> > > 5. **ollama**                                    0.4.7\n",
    "> > > 6. **chromadb**                                 0.6.3\n",
    "> > 5. **ollama Server**                             0.5.4-ipexllm-20250223\n",
    "> > 6. **LLM Model** - **\"IBM Granite 3.1-dense 8B\"** *[ollama run granite3.1-dense:8b] from [https://ollama.com/library/granite3.1-dense]* is a text-only dense LLM trained on over 12 trillion tokens of data from IBM’s granite family of small open LLM. *The IBM Granite 3.1 language models [https://www.ibm.com/granite] are designed for agentic workflows, RAG, text summarization, text analytics and extraction, classification, and content generation.*\n",
    "> > 7. **Embedding Model** - **\"BAAI/llm-embedder\"** *from offcial BAAI hugginface repo [https://huggingface.co/BAAI/llm-embedder]* - *LLM-Embedder [https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder] is small size unified embedding model that support diverse retrieval augmentation needs for LLMs* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176dba72-43f0-46e2-b308-716c6396b2d7",
   "metadata": {},
   "source": [
    "## 2. Key Components and Concepts\n",
    " * **Large Language Model (LLM)** :  A powerful AI model trained on a massive dataset, capable of understanding and generating human-like text.  Examples include Llama 2, Mistral, and others.  In this setup, we'll use Ollama to run the LLM locally.\n",
    " * **Ollama** : A tool for running LLMs locally. It simplifies the process of downloading, managing, and running open-source LLMs on your own hardware.\n",
    " * **LlamaIndex** : A framework that simplifies the process of connecting LLMs to external data sources. It provides tools for data ingestion, indexing, and querying.\n",
    " * **ChromaDB** : A vector database designed for storing and querying embeddings. Embeddings are numerical representations of text, capturing semantic meaning. ChromaDB enables efficient similarity searches, crucial for retrieving relevant documents.\n",
    " * **Embeddings** : Numerical vectors that represent the semantic meaning of text.  Similar texts have similar embeddings.  We'll use an embedding model (potentially also hosted locally via Ollama) to generate these vectors.\n",
    " * **Vector Database** : A specialized database designed to store and efficiently query vector embeddings.  ChromaDB is used here to store and retrieve document embeddings.\n",
    " * **Document Chunks** :  Large documents are often split into smaller chunks for processing and retrieval. This improves retrieval accuracy and reduces computational load.\n",
    " * **Query** : A user's question or request to the knowledge base.\n",
    " * **Retrieval** : The process of identifying relevant document chunks from the knowledge base based on the query.\n",
    " * **Augmentation** : The process of combining the retrieved document chunks with the user's query before sending it to the LLM.\n",
    " * **Prompt Engineering** : Designing effective prompts to guide the LLM towards generating desired responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a090dc9a-88cf-44f8-b5bf-de30ce955ce5",
   "metadata": {},
   "source": [
    "## 3. System Architecture\n",
    "The system follows a typical RAG architecture:\n",
    " 1. **Data Ingestion** : Documents are loaded and processed.\n",
    " 2. **Chunking** : Documents are split into smaller, manageable chunks.\n",
    " 3. **Embedding Generation** : Embeddings are created for each chunk using an embedding model.\n",
    " 4. **Vector Storage** : Embeddings are stored in ChromaDB.\n",
    " 5. **Query Processing** : User queries are embedded.\n",
    " 6. **Retrieval** : Relevant chunks are retrieved from ChromaDB based on embedding similarity.\n",
    " 7. **Augmentation** : The query and retrieved chunks are combined into a prompt.\n",
    " 8. **LLM Inference** : The prompt is sent to the local LLM (via Ollama).\n",
    " 9. **Response Generation** : The LLM generates a response based on the provided context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64dcfe7-7ecf-45ed-b98c-f6710cbb6590",
   "metadata": {},
   "source": [
    "## 4. Implementation Steps and Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4fd294-aaa2-46f3-9a1b-5c898b738f5a",
   "metadata": {},
   "source": [
    "#### Step 1 : Configure logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588d4098-e030-4ff4-9549-b45aca08ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "# You may turn off logging or change levels once testing is complete and everything is working as intended\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s') # You can output to screen if you're using the code in an interactive notebook\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99cc36e-540f-4945-9732-3284bc6f6ce5",
   "metadata": {},
   "source": [
    "#### Step 2 : Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff6a3e0-8fea-45ae-a329-179beb4471c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    Settings,\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from tqdm.notebook import tqdm as log_progress\n",
    "\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922ed295-3da9-4000-9399-170e091cedda",
   "metadata": {},
   "source": [
    "##### ***If you're running the code as an app you can save logs to a timestamped file like this ->***\n",
    "---\n",
    "\n",
    "```Python\n",
    "from logging.handlers import RotatingFileHandler\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "log_file_path= \"./logs\"\n",
    "log_filename= os.path.join(log_file_path, 'RAGBot.log')\n",
    "if not os.path.exists(log_file_path):\n",
    "    os.makedirs(log_file_path)\n",
    "handler = RotatingFileHandler(log_filename, maxBytes=100000,backupCount=3, encoding='utf-8')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cbed34-271f-41fa-8dd9-4366198af2e6",
   "metadata": {},
   "source": [
    "#### Step 3 : Setup default paths and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73c04cf-7c4b-47bd-ad13-f974ce2d2e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory containing your documents\n",
    "DOCUMENT_PATH = \"./data/pdf\"\n",
    "\n",
    "# Specify the file to hold the list of documents indexed\n",
    "INDEXED_FILES_RECORD = os.path.join(DOCUMENT_PATH, 'indexed_files.txt')\n",
    "\n",
    "# Directory to persist ChromaDB\n",
    "PERSIST_DIR = \"./data/storage/chroma_db\"\n",
    "\n",
    "# Set collection name\n",
    "COLLECTION_NAME = \"document_index\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a4ea0-6f4d-4d85-a7af-d09cd8cf4739",
   "metadata": {},
   "source": [
    "#### Step 4 : Setup the Embedding Model and Ollama Local LLM of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8590285-83fb-41e3-8c82-366b00f15ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings control global defaults\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/llm-embedder\")\n",
    "Settings.llm = Ollama(model=\"granite3.1-dense:latest\", request_timeout=360.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1992db36-5c74-4272-b2c2-1bd6d30bdbf8",
   "metadata": {},
   "source": [
    "#### Step 5 : Load Data\n",
    "Our approach for scanning and adding files to vector DB is as follows :\n",
    "> * On first run, scan and load all the files in the DOCUMENT_PATH and create an INDEXED_FILES_RECORD (txt) to store the list of files indexed - *First time run is assumed if no INDEXED_FILES_RECORD is found in DOCUMENT_PATH*\n",
    "> * The INDEXED_FILES_RECORD is then populated with names of currently loaded documents\n",
    "> * This file is then read in subsequent runs and compared with the DOCUMENT_PATH listing to see any new files have been added\n",
    "> * If any new file is found then they are indexed and vectorized and the INDEXED_FILES_RECORD is updated\n",
    "> * This way we can make build our knowldge base as and when more data is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edf67aa-75b2-484a-a757-979ba0a4a2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index file doesnot exist first time. So create an empty file\n",
    "if not os.path.exists(INDEXED_FILES_RECORD):\n",
    "    open(INDEXED_FILES_RECORD, \"w\").close()\n",
    "    # Get all files in the directory\n",
    "    all_files = [f for f in os.listdir(DOCUMENT_PATH) if os.path.isfile(os.path.join(DOCUMENT_PATH, f))]\n",
    "\n",
    "    # Write file names to indexed_files.txt\n",
    "    with open(INDEXED_FILES_RECORD, \"w\") as f:\n",
    "        for file in all_files:\n",
    "            f.write(file + \"\\n\")\n",
    "\n",
    "    print(f\"Indexed {len(all_files)} files from {DOCUMENT_PATH} into {INDEXED_FILES_RECORD}.\")\n",
    "\n",
    "    # Load and preprocess the documents\n",
    "    documents = SimpleDirectoryReader(DOCUMENT_PATH).load_data()\n",
    "    # 'documents' now holds a list of document objects ready for indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe20fb3c-0b91-4be4-ba59-2605f4c92547",
   "metadata": {},
   "source": [
    "#### Step 6 : Set up the ChromaDB client with a persistent storage directory\n",
    "> * Check for PERSIST_DIR , and if it's not available , build a fresh COLLECTION_NAME and store it\n",
    "> * if PERSIST_DIR exists , then Update existing COLLECTION_NAME if there are any new files in DOCUMENT_PATH\n",
    "> * Update INDEXED_FILES_RECORD once all new files are vectorized and added to COLLECTION_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a5fe04-34af-4774-9d94-b881df080dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(PERSIST_DIR):\n",
    "\n",
    "    # Create Chroma client, opt out of telemetry and create our collection\n",
    "    from chromadb.config import Settings\n",
    "    chroma_client = chromadb.PersistentClient(path=PERSIST_DIR,settings=Settings(anonymized_telemetry=False))\n",
    "    chroma_collection = chroma_client.create_collection(COLLECTION_NAME)\n",
    "\n",
    "    # Create vector store and the storage context; the 'collection_name' groups our embeddings.\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    # Build the index from the loaded documents.\n",
    "    index = VectorStoreIndex.from_documents(documents, show_progress=True, storage_context=storage_context)\n",
    "    print(f\"FINISH: Updated Chroma collection / index with new documents now has {chroma_collection.count()} vectors.  Each vector represents one page of a document.\")\n",
    "    # The index now holds the vector embeddings of our documents and is ready for querying.\n",
    "\n",
    "    # Persist the index\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "\n",
    "else:\n",
    "\n",
    "    # Load the existing index\n",
    "    from chromadb.config import Settings\n",
    "    chroma_client = chromadb.PersistentClient(path=PERSIST_DIR,settings=Settings(anonymized_telemetry=False))\n",
    "    chroma_collection= chroma_client.get_collection(COLLECTION_NAME)\n",
    "    print(f\"START: Initial Chroma collection has {chroma_collection.count()} vectors.  Each vector represents one page of a document.\")\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection, mode=\"append\")\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    index = VectorStoreIndex.from_vector_store(\n",
    "        vector_store,\n",
    "        show_progress=True\n",
    "    )\n",
    "    print(\"Loaded existing index from Disk\")\n",
    "\n",
    "    # Load previously indexed files\n",
    "    indexed_files = set()\n",
    "    if os.path.exists(INDEXED_FILES_RECORD):\n",
    "        with open(INDEXED_FILES_RECORD, \"r\") as f:\n",
    "            indexed_files = set(f.read().splitlines())\n",
    "\n",
    "    # Get list of all documents files in the directory and check for new ones\n",
    "    all_docs = set(os.listdir(DOCUMENT_PATH))\n",
    "    new_docs = [f for f in all_docs if f not in indexed_files]\n",
    "\n",
    "    if new_docs:\n",
    "        print(f\"Found {len(new_docs)} new Document(s). Adding to vector store...\")\n",
    "\n",
    "        new_documents = []\n",
    "        for doc in new_docs:\n",
    "            doc_path = os.path.join(DOCUMENT_PATH, doc)\n",
    "            docs = SimpleDirectoryReader(input_files=[doc_path]).load_data() # Load and preprocess the new documents\n",
    "            new_documents.extend(docs) # Collect the new documents\n",
    "\n",
    "        # Build the index from the new documents.\n",
    "        index = VectorStoreIndex.from_documents(new_documents, show_progress=True, storage_context=storage_context)\n",
    "        print(f\"FINISH: Updated Chroma collection / index with new documents now has {chroma_collection.count()} vectors.  Each vector represents one page of a document.\")\n",
    "        # The index now holds the vector embeddings of our new documents and is ready for querying.\n",
    "\n",
    "        # Persist the index\n",
    "        index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "\n",
    "        # Update the indexed files record\n",
    "        with open(INDEXED_FILES_RECORD, \"a\") as f:\n",
    "            for doc in new_docs:\n",
    "                f.write(doc + \"\\n\")\n",
    "\n",
    "        print(\"Vector store updated successfully.\")\n",
    "    else:\n",
    "        print(\"No new files to index.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10a327c-f6fc-40c8-9e0c-59dc72e40642",
   "metadata": {},
   "source": [
    "#### Step 7 : Query and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d267a79a-c76d-4f2e-8051-f765aa9dae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a sample query to test the response from our model\n",
    "query_engine = index.as_query_engine()\n",
    "# Example query\n",
    "query = \"What is the main topic of this document?\"\n",
    "response = query_engine.query(query)\n",
    "\n",
    "# Print the generated response.\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17650e6b-90a2-4279-b596-4870cd99d96d",
   "metadata": {},
   "source": [
    "#### Step 8 : (Optional) Run a simple chatbot interface to ask questions about the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518754c0-864d-408b-881c-38507d2aa8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start interactive query loop\n",
    "while True:\n",
    "    query = input(\"Hi, This is a simple RAGBot. Enter your query (or type 'exit' to quit) : \\n\")\n",
    "\n",
    "    if query.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "    # Keep retrieving relevant responses for the query\n",
    "    response = query_engine.query(query)\n",
    "\n",
    "    # Display the retrieved data\n",
    "    print(\"RAGBot:\\n\", response, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d5f84a-9a05-4916-a2c9-71aa99696c4b",
   "metadata": {},
   "source": [
    "## 5. Explanation of Code Sections\n",
    "---\n",
    " 1. **Logging** : Set up basic logging for debugging.\n",
    " 2. **Imports** : Import necessary libraries.\n",
    " 4. **Configure** : Configure the data folder where documents are stored, the name of the ChomaDB collection and location for the presistent storage \n",
    " 5. **Model Settings** : Initializes the Ollama LLM and embedding model. Replace \"granite3.1-dense:latest\" with the name of the model you have downloaded via ollama pull. This context is used throughout the indexing and querying process.\n",
    " 6. **Data Loading** :  SimpleDirectoryReader loads documents from a specified directory.  LlamaIndex supports various data connectors (e.g., web pages, databases).\n",
    " 7. **ChromaDB Setup and Index Creation/Loading** : The code checks for a persisted index. If it doesn't exist, it creates a ChromaDB client, a collection, a ChromaVectorStore, and a StorageContext. It then builds the VectorStoreIndex from the documents and persists it.  If the index does exist, it's loaded from disk.  This persistence step is crucial for efficiency – you only build the index once.\n",
    " 8. **Querying** : Creates a query_engine from the index and uses it to answer a query.\n",
    " 9. **Chatbot Interface (Optional)** :  Provides a simple loop for interactive querying.\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b83c77e-5a7b-419a-8e22-7f25342f0042",
   "metadata": {},
   "source": [
    "#### 6. Setting up Ollama\n",
    " * **Installation** : Follow the instructions on the Ollama website (https://ollama.ai/) for your operating system.\n",
    " * **Model Download** :  Use the ollama pull <model_name> command to download the desired LLM.  For example, ollama pull llama2.  You'll need to choose a model that is compatible with LlamaIndex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6918deb1-aa1d-4627-a1bc-1d931d91d628",
   "metadata": {},
   "source": [
    "#### 7. Setting up ChromaDB\n",
    " * **Installation** : pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2484da90-2096-46a7-9876-51aafc3cc46a",
   "metadata": {},
   "source": [
    "#### 8. Setting up LlamaIndex\n",
    " * **Installation** : pip install llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36df9c9-3781-4e04-8fe6-115e5c25f1a9",
   "metadata": {},
   "source": [
    "#### 9. Running the Code\n",
    " * **Data** : Place your documents in the \"data\" directory (or adjust the path in the code).\n",
    " * **Dependencies** : Ensure all required libraries are installed.\n",
    " * **Execution** : Run the Python script.  The first run will take longer as the index is built. Subsequent runs will be faster as the index is loaded from disk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a1c534-82ae-4ae4-8bac-457d7c99c269",
   "metadata": {},
   "source": [
    "## 10.  Further Enhancements\n",
    " * **More Advanced Querying** : Explore LlamaIndex's query engine options for more sophisticated retrieval strategies (e.g., keyword-based retrieval, hybrid search).\n",
    " * **Evaluation** : Implement metrics to evaluate the performance of the RAG system.\n",
    " * **Prompt Engineering** : Experiment with different prompt templates to optimize the quality of LLM responses.\n",
    " * **Data Connectors** : Explore LlamaIndex's data connectors for integrating with various data sources.\n",
    " * **Asynchronous Operations** : For larger datasets, consider using asynchronous operations to speed up indexing and querying.\n",
    " * **Fine-tuning** :  For highly specific domains,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
